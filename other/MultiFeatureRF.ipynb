{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from mne import viz\n",
    "from scipy import io, stats\n",
    "from matplotlib import colors, pyplot as plt\n",
    "from sklearn import model_selection, ensemble, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAF_DOSE = 200\n",
    "\n",
    "DATA_PATH = 'C:\\\\Users\\\\Philipp\\\\Documents\\\\Caffeine\\\\Features{dose}\\\\Combined'.format(dose=CAF_DOSE)\n",
    "RESULTS_PATH = 'C:\\\\Users\\\\Philipp\\\\GoogleDrive\\\\Caffeine\\\\results\\\\randomForest{dose}'.format(dose=CAF_DOSE)\n",
    "PROJECT_PATH = '..\\\\data'\n",
    "\n",
    "STAGES = ['AWA', 'AWSL', 'NREM', 'REM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_pos = io.loadmat(os.path.join(PROJECT_PATH, 'Coo_caf'))['Cor'].T\n",
    "sensor_pos = np.array([sensor_pos[1], sensor_pos[0]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(DATA_PATH, 'data_avg.pickle'), 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "with open(os.path.join(DATA_PATH, 'labels_avg.pickle'), 'rb') as file:\n",
    "    y = pickle.load(file)\n",
    "with open(os.path.join(DATA_PATH, 'groups_avg.pickle'), 'rb') as file:\n",
    "    groups = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = np.concatenate([[feature + '-' + str(i) for i in range(20)] for feature in data[STAGES[0]].keys() if 'Perm' not in feature and 'SpecSamp' not in feature])\n",
    "\n",
    "x = {}\n",
    "for stage in STAGES:\n",
    "    x[stage] = []\n",
    "    for feature in data[stage].keys():\n",
    "        if 'Perm' in feature or 'SpecSamp' in feature:\n",
    "            continue\n",
    "        x[stage].append(data[stage][feature])\n",
    "    x[stage] = np.concatenate(x[stage], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 78 samples\n",
      "AWA iteration 0/100\n",
      "AWA iteration 50/100\n",
      "AWA iteration 100/100\n",
      "mean score: 0.5225 \n",
      "\n",
      "Training on 74 samples\n",
      "AWSL iteration 0/100\n",
      "AWSL iteration 50/100\n",
      "AWSL iteration 100/100\n",
      "mean score: 0.56875 \n",
      "\n",
      "Training on 78 samples\n",
      "NREM iteration 0/100\n",
      "NREM iteration 50/100\n",
      "NREM iteration 100/100\n",
      "mean score: 0.57125 \n",
      "\n",
      "Training on 78 samples\n",
      "REM iteration 0/100\n",
      "REM iteration 50/100\n",
      "REM iteration 100/100\n",
      "mean score: 0.5375 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "iterations = 100\n",
    "\n",
    "estimator_dict = {}\n",
    "testing_data_dict = {}\n",
    "\n",
    "for stage in STAGES:\n",
    "    testing_data = []\n",
    "    estimators = []\n",
    "    avg_score = []\n",
    "    \n",
    "    print(f'Training on {len(x[stage])} samples')\n",
    "    \n",
    "    counter = 0\n",
    "    cv = model_selection.LeavePGroupsOut(n_groups=4)\n",
    "    cv_split = list(cv.split(x[stage], y[stage], groups[stage]))\n",
    "    for i in np.random.permutation(len(cv_split)):\n",
    "        train, test = cv_split[i]\n",
    "        if counter % 50 == 0:\n",
    "            print(f'{stage} iteration {counter}/{iterations}')\n",
    "        if counter >= iterations:\n",
    "            break\n",
    "        \n",
    "        clf = ensemble.RandomForestClassifier(n_jobs=-1)\n",
    "\n",
    "        '''\n",
    "        params = {\n",
    "            'n_estimators': [10, 50, 150, 300],\n",
    "            'max_depth': [10, 40, None],\n",
    "            'criterion': ['gini', 'entropy'],\n",
    "            'bootstrap': [True, False],\n",
    "            'max_features': ['auto', 'sqrt', 'log2'],\n",
    "            'min_samples_leaf': [5, 25, 60]\n",
    "        }\n",
    "        '''\n",
    "\n",
    "        params = {\n",
    "            'n_estimators': [50],\n",
    "            'max_depth': [10],\n",
    "            'criterion': ['gini'],\n",
    "            #'bootstrap': [True, False],\n",
    "            #'max_features': ['auto', 'sqrt', 'log2'],\n",
    "            #'min_samples_leaf': [1, 5],\n",
    "            'class_weight': ['balanced_subsample']\n",
    "        }\n",
    "        \n",
    "        kfold_inner = model_selection.GroupKFold(n_splits=10)\n",
    "        inner_cross_validation_split = kfold_inner.split(x[stage][train],\n",
    "                                                         y[stage][train],\n",
    "                                                         groups[stage][train])\n",
    "\n",
    "        grid_search = model_selection.GridSearchCV(estimator=clf,\n",
    "                                                   param_grid=params,\n",
    "                                                   cv=inner_cross_validation_split,\n",
    "                                                   iid=False,\n",
    "                                                   refit=True,\n",
    "                                                   n_jobs=-1)\n",
    "        grid_search.fit(x[stage][train], y[stage][train], groups[stage][train])\n",
    "        \n",
    "        testing_data.append((x[stage][test], y[stage][test]))\n",
    "        estimators.append(grid_search.best_estimator_)\n",
    "        avg_score.append(grid_search.best_estimator_.score(x[stage][test], y[stage][test]))\n",
    "        counter += 1\n",
    "\n",
    "    testing_data_dict[stage] = testing_data\n",
    "    estimator_dict[stage] = estimators\n",
    "    \n",
    "    print('mean score:', np.mean(avg_score), '\\n')\n",
    "    \n",
    "with open(os.path.join(RESULTS_PATH, 'estimators.pickle'), 'wb') as file:\n",
    "    pickle.dump(estimator_dict, file)\n",
    "with open(os.path.join(RESULTS_PATH, 'testing_data.pickle'), 'wb') as file:\n",
    "    pickle.dump(testing_data_dict, file)\n",
    "with open(os.path.join(RESULTS_PATH, 'feature_names.pickle'), 'wb') as file:\n",
    "    pickle.dump(feature_names, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
