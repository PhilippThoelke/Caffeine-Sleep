{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import datetime\n",
    "import numpy as np\n",
    "from keras import utils, models, layers, backend, callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAF_DOSE = 200\n",
    "STAGE = 'NREM'\n",
    "TEST_SUBJECT_COUNT = 5\n",
    "\n",
    "DATA_PATH = 'C:\\\\Users\\\\Philipp\\\\Documents\\\\Caffeine\\\\raw_eeg{dose}'.format(dose=CAF_DOSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawGenerator(utils.Sequence):\n",
    "    \n",
    "    def __init__(self, files_caf, files_plac, files_per_batch, permute_labels=False):\n",
    "        self.files_caf, self.files_plac = np.array(files_caf), np.array(files_plac)\n",
    "        self.files_per_batch = files_per_batch\n",
    "        self.permute_labels = permute_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(min(len(self.files_caf), len(self.files_plac)) / self.files_per_batch))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = idx * self.files_per_batch\n",
    "        end = (idx + 1) * self.files_per_batch\n",
    "        \n",
    "        batch_files_caf = self.files_caf[start:end]\n",
    "        batch_files_plac = self.files_plac[start:end]\n",
    "        \n",
    "        batch_data_list_caf = [np.load(file) for file in batch_files_caf]\n",
    "        samples_per_file_caf = [x.shape[0] for x in batch_data_list_caf]\n",
    "        \n",
    "        batch_data_list_plac = [np.load(file) for file in batch_files_plac]\n",
    "        samples_per_file_plac = [x.shape[0] for x in batch_data_list_plac]\n",
    "        \n",
    "        batch_x = np.concatenate(batch_data_list_caf + batch_data_list_plac, axis=0)\n",
    "        batch_y = np.array([[1, 0]] * np.sum(samples_per_file_caf) + [[0, 1]] * np.sum(samples_per_file_plac))\n",
    "        \n",
    "        batch_x = utils.normalize(batch_x, axis=0)\n",
    "        \n",
    "        if self.permute_labels:\n",
    "            labels = [[0, 1]] * (len(batch_y) // 2) + [[1, 0]] * (len(batch_y) // 2)\n",
    "            if len(labels) < len(batch_y):\n",
    "                labels += [[1, 0]]\n",
    "            batch_y = np.array(labels)\n",
    "            return batch_x[np.random.permutation(batch_x.shape[0])], batch_y[np.random.permutation(batch_y.shape[0])]\n",
    "        else:\n",
    "            perm = np.random.permutation(batch_x.shape[0])\n",
    "            return batch_x[perm], batch_y[perm]\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.files_caf = self.files_caf[np.random.permutation(len(self.files_caf))]\n",
    "        self.files_plac = self.files_plac[np.random.permutation(len(self.files_plac))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found raw EEG data for 31 subjects, leaving 5 for validation\n",
      "Caffeine data files: 346, placebo data files: 394\n"
     ]
    }
   ],
   "source": [
    "def get_subject_id(path):\n",
    "    return re.match('\\S\\d+', path.split(os.sep)[-1].split('_')[0])[0]\n",
    "    \n",
    "caf_files = glob.glob(os.path.join(DATA_PATH, f'*{STAGE}*CAF*'))\n",
    "plac_files = glob.glob(os.path.join(DATA_PATH, f'*{STAGE}*PLAC*'))\n",
    "\n",
    "caf_subjects = set([get_subject_id(file) for file in caf_files])\n",
    "plac_subjects = set([get_subject_id(file) for file in plac_files])\n",
    "\n",
    "subjects = caf_subjects & plac_subjects\n",
    "\n",
    "test_subjects = set()\n",
    "while len(test_subjects) < TEST_SUBJECT_COUNT:\n",
    "    test_subjects.add(np.random.choice(list(subjects)))\n",
    "\n",
    "print(f'Found raw EEG data for {len(subjects)} subjects, leaving {len(test_subjects)} for validation')\n",
    "print(f'Caffeine data files: {len(caf_files)}, placebo data files: {len(plac_files)}')\n",
    "    \n",
    "caf_files_train = [file for file in caf_files if get_subject_id(file) not in test_subjects]\n",
    "plac_files_train = [file for file in plac_files if get_subject_id(file) not in test_subjects]\n",
    "caf_files_test = [file for file in caf_files if get_subject_id(file) in test_subjects]\n",
    "plac_files_test = [file for file in plac_files if get_subject_id(file) in test_subjects]\n",
    "\n",
    "train = RawGenerator(caf_files_train, plac_files_train, 3)\n",
    "test = RawGenerator(caf_files_test, plac_files_test, 3, permute_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.983938323160935 50.016061676839065\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[sum(train[i][1] == [0, 1])[0], sum(train[i][1] == [1, 0])[0]] for i in range(len(train))])\n",
    "print(a[:,0].sum() / a.sum() * 100, a[:,1].sum() / a.sum() * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 1277, 16)          4816      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 212, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 68, 8)             1288      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 11, 8)             0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 88)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                2848      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 9,018\n",
      "Trainable params: 9,018\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "backend.clear_session()\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv1D(filters=16, kernel_size=15, strides=4, activation='relu', input_shape=(5120, 20)))\n",
    "model.add(layers.MaxPooling1D(pool_size=6))\n",
    "model.add(layers.Conv1D(filters=8, kernel_size=10, strides=3, activation='relu'))\n",
    "model.add(layers.MaxPooling1D(pool_size=6))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(2, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Philipp\\Anaconda3\\envs\\mne\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      "99/99 [==============================] - 313s 3s/step - loss: 0.6940 - acc: 0.5046 - val_loss: 0.6931 - val_acc: 0.5032\n",
      "Epoch 2/100\n",
      "55/99 [===============>..............] - ETA: 1:59 - loss: 0.6930 - acc: 0.5011"
     ]
    }
   ],
   "source": [
    "name = datetime.datetime.now().strftime('conv_%Y-%m-%d_%H-%M-%S')\n",
    "tensorboard = callbacks.TensorBoard(log_dir='..\\\\results\\\\logs\\\\' + name)\n",
    "\n",
    "model.fit_generator(generator=train,\n",
    "                    epochs=100,\n",
    "                    validation_data=test,\n",
    "                    max_queue_size=6,\n",
    "                    workers=6,\n",
    "                    callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training accuracy:', model.evaluate_generator(train)[1])\n",
    "print('Testing accuracy: ', model.evaluate_generator(test)[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
